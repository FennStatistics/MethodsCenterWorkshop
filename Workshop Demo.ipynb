{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e77166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading \u001b[1mcuda/12.1\u001b[22m\u001b[m\n",
      "  \u001b[94mLoading requirement\u001b[0m: cudnn/8.9.1-cu12.x\u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!source ../sel-env/bin/activate\n",
    "!module load cuda/12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets evaluate accelerate torch torchvision torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de402d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402df2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/tsuehr/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3b78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "softmax = torch.nn.Softmax(-1)\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd3c3f",
   "metadata": {},
   "source": [
    "## Tokens and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989decdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "\n",
    "len(tokenizer)\n",
    "tokenizer.encode(\"hello, my name is tom\")\n",
    "# tokenizer.decode(1)\n",
    "# tokenizer.decode(22172)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39091e01",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a44c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "# Set the logging level to ERROR to avoid seeing warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Load a pretrained BERT model and its tokenizer from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example sentence to encode\n",
    "sentence = \"I love natural language processing\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "print(f\"Prompt: {sentence}\")\n",
    "print(f\"Tokens: {inputs.input_ids}\")\n",
    "\n",
    "# Forward pass through the model to get the hidden states (embeddings)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The embeddings are in the 'last_hidden_state' output\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Show the embeddings for the tokens in the sentence\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "print(\"Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c52807",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#We can use this if we want the embeddings of our model as output\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "\n",
    "#or meta-llama/Llama-3.2-1B\n",
    "#or meta-llama/Llama-3.1-8B-Instruct\n",
    "#or \"gpt2\"\n",
    "#or \"TinyLlama/TinyLlama_v1.1\"\n",
    "\n",
    "def get_logprob(model,tokenizer,prompt, completion):\n",
    "    # Encode both the prompt and the prompt + completion\n",
    "    # We need to encode the prompt to know how many tokens to mask out\n",
    "    prompt_tokenized = tokenizer.encode(prompt)\n",
    "    print(prompt)\n",
    "    print(prompt_tokenized)\n",
    "    prompt_completion_tokenized = tokenizer.encode(prompt + completion)\n",
    "    print(prompt + completion)\n",
    "    print(prompt_completion_tokenized)\n",
    "    \n",
    "    # The input_ids are simply the tokenized prompt + completion\n",
    "    input_ids = torch.tensor([prompt_completion_tokenized])\n",
    "    print(input_ids)\n",
    "\n",
    "    # The labels are the same as the input_ids, but with the prompt tokens masked out\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    labels[:, :len(prompt_tokenized)] = -100 # cross entropy loss ignores labels set to -100 (we mask the prompt)\n",
    "    \n",
    "    # Pass the input_ids and labels to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "    \n",
    "    # Extract the log probability from the model output (e.g., the loss)\n",
    "    loss = outputs.loss.item()\n",
    "    logprob = -loss\n",
    "    return logprob\n",
    "\n",
    "def get_next_word(model,tokenizer,prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')  # Return as a tensor\n",
    "\n",
    "    # Generate the next token\n",
    "    with torch.no_grad():\n",
    "        #For text generation, we use the .generate() method\n",
    "        outputs = model.generate(input_ids, max_new_tokens=1, do_sample=False)  # Predict the next token\n",
    "#         outputs = model.generate(input_ids, max_new_tokens=1, do_sample=True, temperature=0.7)\n",
    "\n",
    "    # Decode the output to get the predicted next word\n",
    "    print(outputs[0])\n",
    "    next_word = tokenizer.decode(outputs[0][-1], skip_special_tokens=False)\n",
    "\n",
    "    return next_word\n",
    "\n",
    "def get_answer_option_probabilities(model,tokenizer,prompt,options):\n",
    "    # Encode both the prompt and the prompt + completion\n",
    "    # We need to encode the prompt to know how many tokens to mask out\n",
    "    prompt_tokenized = tokenizer.encode(prompt)\n",
    "    \n",
    "    # The input_ids are simply the tokenized prompt + completion\n",
    "    input_ids = torch.tensor([prompt_tokenized])\n",
    "    # The labels are the same as the input_ids, but with the prompt tokens masked out\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Pass the input_ids and labels to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        \n",
    "    logits = outputs.logits[:,-1,:][0] # output.logits has the shape (batch_size, sequence_length, vocab_size)\n",
    "    \n",
    "    option_token_ids = np.array([i[-1] for i in tokenizer(list(options))[\"input_ids\"]])\n",
    "    probs = softmax(logits)[option_token_ids]\n",
    "    print(options)\n",
    "    print(probs)\n",
    "    print(f\"Most likely answer is: {options[torch.argmax(probs)]}\")\n",
    "#     predicted_token_id = torch.argmax(logits, dim=-1)\n",
    "#     print(tokenizer.decode(predicted_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The capital of Spain is\"\n",
    "completions = [\" Madrid\", \" Barcelona\", \" Paris\", \" Rome\"]\n",
    "completion = \" Madrid\"\n",
    "\n",
    "logprob = get_logprob(model,tokenizer,prompt,completion)\n",
    "print(f\"Log probability of '{completion}' given ´´{prompt}´´: {logprob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = get_next_word(model,tokenizer,prompt)\n",
    "print(f\"The predicted next word is :{next_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee4139",
   "metadata": {},
   "source": [
    "### MC-Question Answering (MCQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of Spain?\" + '\\n' + 'A:Madrid, B:Barcelona, C:Paris, D: Rome' + '\\nAnswer'\n",
    "options= ['A','B','C','D']\n",
    "get_answer_option_probabilities(model,tokenizer,prompt,options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a3bcd",
   "metadata": {},
   "source": [
    "### A look inside Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att =  GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            rope_base=cfg[\"rope_base\"],\n",
    "            rope_config=cfg[\"rope_freq\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.norm2 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16))   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b47fa2",
   "metadata": {},
   "source": [
    "### Pseudocode for supervised learning with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode for supervised learning\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x, y in data:\n",
    "\n",
    "#         # Forward pass\n",
    "#         prediction = model(x)\n",
    "\n",
    "#         # Calculate loss\n",
    "#         loss = criterion(prediction, y) # e.g. binary cross entropy loss\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc402ba",
   "metadata": {},
   "source": [
    "### Pseudocode for Reinforcement Learning with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8acb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = environment.reset() #reset the world\n",
    "    episode_reward = 0\n",
    "    for t in range(update_steps):\n",
    "        # sample an action according to the current policy\n",
    "        # sampling an action is like sampling a token\n",
    "        action, log_prob = ppo_agent.policy.act(state)\n",
    "        \n",
    "        # take action and get reward and next state\n",
    "        # the reward is learned as a function of user rankings of different outputs\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        value = ppo_agent.policy.forward(state)[1].item()\n",
    "\n",
    "        # Store transition\n",
    "        ppo_agent.store_transition((state, action, reward, done, log_prob.item(), value))\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    # Update PPO after every episode\n",
    "    ppo_agent.update()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee3a36",
   "metadata": {},
   "source": [
    "### Fine-Tuning with axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/axolotl-ai-cloud/axolotl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f359d70",
   "metadata": {},
   "source": [
    "### Fine-Tuning with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496015c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Model:\n",
    "    def __init__(self, model_name='gpt2', **kwargs):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True, **kwargs)\n",
    "        self.max_length = self.tokenizer.model_max_length\n",
    "\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\" Obtain the vector embedding of a text by taking the hidden state of the last layer of the model\n",
    "        corresponding to the last token.\n",
    "\n",
    "        Inputs:\n",
    "        - text: a string\n",
    "\n",
    "        Returns:\n",
    "        - embedding: a numpy array of shape (n,), where n is the dimensionality of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        # tokenize\n",
    "        input_ids = self.tokenizer(text).input_ids\n",
    "\n",
    "        # truncate from the left such that we keep the QA template if there is one\n",
    "        input_ids = input_ids[-min(len(input_ids), self.max_length):]\n",
    "\n",
    "        # convert to torch tensor and add batch dimension\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "\n",
    "        last_layer = outputs['hidden_states'][-1]\n",
    "        last_layer_last_token = last_layer[0, -1]\n",
    "        embedding = last_layer_last_token.cpu().double().numpy()\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "def clopper_pearson(n_correct, n_trials, alpha=0.05):\n",
    "    lower_ci = beta.ppf(alpha / 2, n_correct, n_trials - n_correct + 1)\n",
    "    upper_ci = beta.ppf(1 - alpha / 2, n_correct + 1, n_trials - n_correct)\n",
    "    return lower_ci, upper_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a718e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/tsuehr/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Embedding_Model(model_name='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba4a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4069d98084440e8498974822a8b276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/51.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c928a45609a496e89de97847eaf443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d704ee1f25854e708623938172d5f896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val-00000-of-00001.parquet:   0%|          | 0.00/12.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6a0dbe67a648fab1398a38668e6e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/25.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8b6f1ac94e4ff8a2c88a8b7d1a9acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9686d5fdc48b4658bfc631cdd0d56eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36825bb4f5d6496daffe9526aaa302d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1773 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = datasets.load_dataset('ricdomolm/lawma-tasks', 'sc_issuearea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba51650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example court opinion:\n",
      "----------------\n",
      "ALABAMA et al. v. PUGH et al.\n",
      "No. 77-1107.\n",
      "Decided July 3, 1978\n",
      "Per Curiam.\n",
      "Respondents, inmates or former inmates of the Alabama prison system, sued petitioners, who include the State of Alabama and the Alabama Board of Corrections as well as a number of Alabama officials responsible for the administration of its prisons, alleging that conditions in Alabama prisons constituted cruel and unusual punishment in violation of the Eighth and Fourteenth Amendments. The United States District Court agreed and issued an order prescribing measures designed to eradicate cruel and unusual punishment in the Alabama prison system. The Court of Appeals for the Fifth Circuit affirmed but modified some aspects of the order which it believed exceeded the limits of the appropriate exercise of the court's remedial powers. 559 F. 2d 283.\n",
      "Among the claims raised here by petitioners is that the issuance of a mandatory injunction against the State of Alabama and the Alabama Board of Corrections is unconstitutional because the Eleventh Amendment prohibits federal courts from entertaining suits by private parties against States and their agencies. The Court of Appeals did not address this contention, perhaps because it was of the view that in light of the numerous individual defendants in the case dismissal as to these two defendants would not affect the scope of the injunction. There can be no doubt, however, that suit against the State and its Board of Corrections is barred by the Eleventh Amendment, unless Alabama has consented to the filing of such a suit. Edelman v. Jordan, 415 U. S. 651 (1974); Ford Motor Co. v. Department of Treasury, 323 U. S. 459 (1945); Worcester County Trust Co. v. Riley, 302 U. S. 292 (1937). Respondents do not contend that Alabama has consented to this suit, and it appears that no consent could be given under Art. I, § 14, of the Alabama Constitution, which provides that “the State of Alabama shall never be made a defendant in any court of law or equity.” Moreover, the question of the State’s Eleventh Amendment immunity is not merely academic. Alabama has an interest in being dismissed from this action in order to eliminate the danger of being held, in contempt if it should fail to comply with the mandatory injunction. Consequently, we grant the petition for certio-rari limited to Question 2 presented by petitioners, reverse the judgment in part, and remand the case to the Court of Appeals with instructions to order the dismissal of the State of Alabama and the Alabama Board of Corrections from this action.\n",
      "So ordered.\n",
      "Mr. Justice Brennan and Mr. Justice Marshall dissent.\n",
      "Respondents contend that petitioners failed to raise the Eleventh Amendment issue in the District Court. The Court held in Edelman v. Jordan, 415 U. S. 651, 678 (1974), however, that “the Eleventh Amendment defense sufficiently partakes of the nature of a jurisdictional bar so that it need not be raised in the trial court . . . .”\n",
      "“Whether the mandatory injunction issued against the State of Alabama and the Alabama Board of Corrections violates the State’s Eleventh Amendment immunity or exceeds the jurisdiction granted federal courts by 42 U. S. C. § 1983.”\n",
      "----------------\n",
      "\n",
      "\n",
      "Question: What is the issue area of the decision?\n",
      "----------------\n",
      "Classes: ['Criminal Procedure', 'Civil Rights', 'First Amendment', 'Due Process', 'Privacy', 'Attorneys', 'Unions', 'Economic Activity', 'Judicial Power', 'Federalism', 'Interstate Relations', 'Federal Taxation', 'Miscellaneous', 'Private Action']\n"
     ]
    }
   ],
   "source": [
    "#custom data format is\n",
    "#{opinion:\"...\", question: \"...\", choices:[...]}\n",
    "\n",
    "# print example\n",
    "print('Example court opinion:')\n",
    "print(\"----------------\")\n",
    "print(task['train'][0]['opinion'])\n",
    "print(\"----------------\")\n",
    "print(\"\\n\\nQuestion:\", task['train'][0]['question'])\n",
    "print(\"----------------\")\n",
    "print(\"Classes:\", task['train'][0]['choices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e4e42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b9b97c4a6845ea85242977e2aaf814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7977b47879d44e50b044cfb54a7edc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# subsample the training and test sets to make training and evaluation faster\n",
    "train_set = task['train'].shuffle(seed=1).select(range(500))\n",
    "test_set = task['test'].shuffle(seed=1).select(range(200))\n",
    "\n",
    "# only include the first 3500 characters of the opinion, to make training faster\n",
    "train_set = train_set.map(lambda x: {'opinion': x['opinion'][:3500]})\n",
    "test_set = test_set.map(lambda x: {'opinion': x['opinion'][:3500]})\n",
    "\n",
    "# get the labels of each example\n",
    "train_labels = [example['answer'][0] for example in train_set]\n",
    "test_labels = [example['answer'][0] for example in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb64265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████████████▎   | 477/500 [02:09<00:06,  3.61it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1358 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 500/500 [02:15<00:00,  3.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = np.stack([model.get_embedding(example['opinion']) for example in tqdm(train_set)])\n",
    "test_embeddings = np.stack([model.get_embedding(example['opinion']) for example in tqdm(test_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c20399e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the majority class classifier: 0.236\n",
      "Accuracy of our classifier: 0.305 (0.242, 0.374)\n"
     ]
    }
   ],
   "source": [
    "majority_class = max(set(train_labels), key=train_labels.count)\n",
    "accuracy_majority = train_labels.count(majority_class) / len(train_labels)\n",
    "print(f\"Accuracy of the majority class classifier: {accuracy_majority:.3f}\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_performance(train_embeddings, test_embeddings, tol=1e-3):\n",
    "    clf = LogisticRegression(max_iter=100000, tol=tol)\n",
    "\n",
    "    clf = clf.fit(train_embeddings, train_labels)\n",
    "    y_pred = clf.predict(test_embeddings)\n",
    "\n",
    "    n_trials = len(test_labels)\n",
    "    n_correct = sum(y_pred == test_labels)\n",
    "\n",
    "    accuracy = n_correct / n_trials\n",
    "    lower_ci, upper_ci = clopper_pearson(n_correct, n_trials)\n",
    "    return accuracy, (lower_ci, upper_ci)\n",
    "\n",
    "accuracy, (lower_ci, upper_ci) = get_performance(train_embeddings, test_embeddings, tol=1e-4)\n",
    "\n",
    "### Delete everything above\n",
    "\n",
    "print(f\"Accuracy of our classifier: {accuracy:.3f} ({lower_ci:.3f}, {upper_ci:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
